{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a24119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or more data files not found. Please check paths.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tesla_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more data files not found. Please check paths.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Exit or handle error appropriately\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# --- 3. PREPROCESS DATA ---\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Convert 'Date' columns to datetime objects\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m tesla_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mtesla_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m nifty_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(nifty_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m gold_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(gold_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tesla_data' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 1. IMPORT LIBRARIES ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- 2. LOAD & MERGE DATA (Assuming files are in the same directory or provide full path) ---\n",
    "# It's better to provide full, absolute paths if the notebook is in a different folder.\n",
    "try:\n",
    "    tesla_data = pd.read_csv('C:/Users/hp/Downloads/Tesla.csv')\n",
    "    nifty_data = pd.read_csv('C:/Users/hp/Downloads/NIFTY 50 - HistoricalPE_PBDIV_Data.csv')\n",
    "    gold_data = pd.read_csv('C:/Users/hp/Downloads/Gold price INR.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"One or more data files not found. Please check paths.\")\n",
    "    # Exit or handle error appropriately\n",
    "    \n",
    "# --- 3. PREPROCESS DATA ---\n",
    "# Convert 'Date' columns to datetime objects\n",
    "tesla_data['Date'] = pd.to_datetime(tesla_data['Date'], format='%m/%d/%Y')\n",
    "nifty_data['Date'] = pd.to_datetime(nifty_data['Date'], format='%d %b %Y')\n",
    "gold_data['Date'] = pd.to_datetime(gold_data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = pd.merge(tesla_data, nifty_data, on='Date', how='outer')\n",
    "merged_data = pd.merge(merged_data, gold_data, on='Date', how='outer')\n",
    "merged_data.sort_values('Date', inplace=True)\n",
    "merged_data.fillna(method='ffill', inplace=True) # Forward fill to handle missing values\n",
    "\n",
    "# --- 4. FEATURE ENGINEERING ---\n",
    "# Using a simplified but effective set of features\n",
    "merged_data['Daily Return'] = merged_data['Close'].pct_change()\n",
    "merged_data['5-Day MA'] = merged_data['Close'].rolling(window=5).mean()\n",
    "merged_data['20-Day MA'] = merged_data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Define the features to be used for training the model\n",
    "features_to_use = [\n",
    "    'Close', 'Open', 'High', 'Low', 'Volume', \n",
    "    'P/E', 'P/B', 'INR', 'Daily Return', '5-Day MA', '20-Day MA'\n",
    "]\n",
    "\n",
    "# Create a clean DataFrame for training, ensure all are float\n",
    "data_for_training = merged_data[features_to_use].astype(float)\n",
    "\n",
    "# IMPORTANT: Drop any rows with NaN values created during feature engineering (like the first few MAs)\n",
    "data_for_training.dropna(inplace=True)\n",
    "\n",
    "print(\"Training data shape after feature engineering and dropping NaNs:\", data_for_training.shape)\n",
    "print(\"Features being used:\", data_for_training.columns.tolist())\n",
    "\n",
    "\n",
    "# --- 5. SCALE & PREPARE DATA FOR LSTM ---\n",
    "# Scale all features to a range of 0 to 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data_for_training)\n",
    "\n",
    "# Create a separate scaler ONLY for the 'Close' price (our target).\n",
    "# This is essential for accurately converting predictions back to actual dollar values.\n",
    "close_price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "close_price_scaler.fit(data_for_training[['Close']])\n",
    "\n",
    "# Function to create time-series data\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(time_step, len(dataset)):\n",
    "        X.append(dataset[i-time_step:i, :]) # All feature columns for X\n",
    "        Y.append(dataset[i, 0])             # Just the 'Close' price (column 0) for Y\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Use a 60-day lookback period\n",
    "time_step = 60\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# --- 6. SPLIT DATA INTO TRAINING AND TESTING SETS ---\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# --- 7. BUILD, TRAIN, AND SAVE THE LSTM MODEL ---\n",
    "num_features = X_train.shape[2] # Get the number of features from the data\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(time_step, num_features))) # Layer 1\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(50, return_sequences=False)) # Layer 2\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1)) # Output layer\n",
    "\n",
    "# Use a slightly lower learning rate for more stable convergence\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# --- 8. EVALUATE THE MODEL ---\n",
    "# Make predictions on the test set\n",
    "test_predictions_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions to get actual prices\n",
    "test_predictions = close_price_scaler.inverse_transform(test_predictions_scaled)\n",
    "\n",
    "# Inverse transform the actual test values for comparison\n",
    "y_test_actual = close_price_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = math.sqrt(mean_squared_error(y_test_actual, test_predictions))\n",
    "print(f\"\\nModel Test RMSE: ${rmse:.2f}\")\n",
    "\n",
    "# --- 9. SAVE THE FINAL MODEL ---\n",
    "# This is the file you will copy to your Django project\n",
    "model.save('stock_model.keras')\n",
    "print(\"\\nModel successfully saved as 'stock_model.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
